{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unit 2 Sprint 1: Intro to Predictive Modeling - Assignment",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shmilyface/DS-Unit-2-Linear-Models/blob/master/Stephanie%20Miller%3A%20Unit_2_Sprint_1_Intro_to_Predictive_Modeling_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvBLOCX9ZTDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import libraries\n",
        "import pandas as pd\n",
        "#read csv\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/nyc/nyc-rent-2016.csv')\n",
        "#assert acts as a sanity check\n",
        "#if this is true, carry on. If not, kill the program.\n",
        "#in this case, we're saying hey, if that df isn't in the shape I thought it was,\n",
        "#sound the alarm \n",
        "assert df.shape == (48300, 34)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqBfhRtVaDbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade pandas-profiling"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIRqt1Ndae-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install jupyter_http_over_ws"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHnqT63AZ2L2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import profiling\n",
        "import pandas_profiling\n",
        "#check version (should be 2.1.0)\n",
        "pandas_profiling.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZOjjdt7aeY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define problem\n",
        "#regression\n",
        "#tabular\n",
        "\n",
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi1C8pOmZvH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#profile report has interactive mouse-overs for graphs that show\n",
        "#percentage points *swoon* \n",
        "#also it tells us literally everything about the data set and could completely\n",
        "#replace several lessons in Unit 1. \n",
        "df.profile_report()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0zpfdTLc4UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#look at months \n",
        "df['created'].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivnmYpACc_b1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#change the format of created column to datetime\n",
        "#infer allows the function to predict what type of time to use\n",
        "#use last month for testing \n",
        "df['created'] = pd.to_datetime(df['created'], infer_datetime_format=True)\n",
        "df['created'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QulVX7xdTkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#feature engineering\n",
        "#change date to month numerical\n",
        "df['month'] = df['created'].dt.month"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA2FhAUrdcpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creation of test and train model, using df.month\n",
        "#train all months under 6\n",
        "train = df[df['month'] < 6]\n",
        "#test only the 6 month\n",
        "test = df[df['month'] == 6]\n",
        "#assert train + test == df\n",
        "assert train.shape[0] + test.shape[0] == df.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ota6_0Ugdq1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#more than half the data is train\n",
        "#note first and last to confirm df is under 6\n",
        "train['created'].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbaWeAZRduuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#final amount saved for test (using time allows us to check accuracy for future\n",
        "#predictions)\n",
        "#note dates are 6-1 to 6-29\n",
        "test['created'].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOjjCehod8cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#how would we do if we just used the mean? (equiv. to guessing)\n",
        "\n",
        "#check the mean of training data to establish baseline for model \n",
        "train['price'].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEnoQOHIeC-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a small df using .head(10), column test.price\n",
        "first10 = test[['price']].head(10)\n",
        "first10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLyRNFLreQLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#basic prediction of just using mean for the whole set to try and predict a price. \n",
        "first10['predicted'] = [3432]*10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNUvICy3eXgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#in order to get error margin, subtract predicted from price. \n",
        "#predicted is target vector, error is accuracy of prediction \n",
        "#helps create an error metric for our model to be built on\n",
        "first10['error'] = first10['price'] - first10['predicted']\n",
        "first10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k05ChFL3edqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this does not actually work to accurately give mean\n",
        "#it tells us we're off by -250.9 on average for each prediction\n",
        "#we can easily check the df to see this isn't accurate\n",
        "first10['error'].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiR4wGZAep7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#absolute errors accounts for outliers and extreme data\n",
        "first10['error'].abs().mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeYjODCu32Dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#basically we just turn all errors into positive integers\n",
        "first10['absolute_error'] = first10['error'].abs()\n",
        "first10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nphJf7z_feYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mean absolute error regression loss\n",
        "'''\n",
        "mean_absolute_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
        "    Mean absolute error regression loss\n",
        "    \n",
        "    Read more in the :ref:`User Guide <mean_absolute_error>`.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
        "        Ground truth (correct) target values.\n",
        "    \n",
        "    y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
        "        Estimated target values.\n",
        "    \n",
        "    sample_weight : array-like of shape = (n_samples), optional\n",
        "        Sample weights.\n",
        "    \n",
        "    multioutput : string in ['raw_values', 'uniform_average']\n",
        "        or array-like of shape (n_outputs)\n",
        "        Defines aggregating of multiple output values.\n",
        "        Array-like value defines weights used to average errors.\n",
        "    \n",
        "        'raw_values' :\n",
        "            Returns a full set of errors in case of multioutput input.\n",
        "    \n",
        "        'uniform_average' :\n",
        "            Errors of all outputs are averaged with uniform weight.\n",
        "    \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    loss : float or ndarray of floats\n",
        "        If multioutput is 'raw_values', then mean absolute error is returned\n",
        "        for each output separately.\n",
        "        If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
        "        weighted average of all output errors is returned.\n",
        "    \n",
        "        MAE output is non-negative floating point. The best value is 0.0.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> from sklearn.metrics import mean_absolute_error\n",
        "    >>> y_true = [3, -0.5, 2, 7]\n",
        "    >>> y_pred = [2.5, 0.0, 2, 8]\n",
        "    >>> mean_absolute_error(y_true, y_pred)\n",
        "    0.5\n",
        "    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
        "    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
        "    >>> mean_absolute_error(y_true, y_pred)\n",
        "    0.75\n",
        "    >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
        "    array([0.5, 1. ])\n",
        "    >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
        "    ... # doctest: +ELLIPSIS\n",
        "    0.85...\n",
        "'''\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "#compare the true prices with the predicted prices\n",
        "mean_absolute_error(first10['price'], first10['predicted'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03de3uRvex_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#true target values (what were these prices actually)\n",
        "y_test = test['price']\n",
        "#what did you estimate or predict\n",
        "y_pred = [train['price'].mean()]*len(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vLVaeL-e9Y1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# of observations \n",
        "print(\"y_pred length:\", len(y_pred))\n",
        "print(\"y_test length:\", len(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAL4FILMe_Zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#runs a comparison between model and tests\n",
        "#note difference between prior and after mae results\n",
        "#on average, we're off on predictions by 1052.67\n",
        "\n",
        "#next: how quickly can we get that number lower?\n",
        "mean_absolute_error(y_pred, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceygc3TofYsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}