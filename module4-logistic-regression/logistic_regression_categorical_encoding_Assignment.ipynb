{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic_regression_categorical_encoding_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MAL3X-01/DS-Unit-2-Linear-Models/blob/master/module4-logistic-regression/logistic_regression_categorical_encoding_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_9p4HPnobGd",
        "colab_type": "text"
      },
      "source": [
        "# Assignment\n",
        "- Learn about the mathematics of Logistic Regression by watching Aaron Gallant's [video #1](https://www.youtube.com/watch?v=pREaWFli-5I) (12 minutes) & [video #2](https://www.youtube.com/watch?v=bDQgVt4hFgY) (9 minutes).\n",
        "- Start a clean notebook.\n",
        "- Do train/validate/test split with the Tanzania Waterpumps data.\n",
        "- Begin to explore and clean the data. For ideas, refer to [The Quartz guide to bad data](https://github.com/Quartz/bad-data-guide),  a \"reference to problems seen in real-world data along with suggestions on how to resolve them.\" One of the issues is [\"Zeros replace missing values.\"](https://github.com/Quartz/bad-data-guide#zeros-replace-missing-values)\n",
        "- Select different numeric and categorical features. \n",
        "- Do one-hot encoding. (Remember it may not work with high cardinality categoricals.)\n",
        "- Scale features.\n",
        "- Use scikit-learn for logistic regression.\n",
        "- Get your validation accuracy score.\n",
        "- Get and plot your coefficients.\n",
        "- Submit your predictions to our Kaggle competition.\n",
        "- Commit your notebook to your fork of the GitHub repo.\n",
        "\n",
        "## Stretch Goals\n",
        "- Begin to visualize the data.\n",
        "- Try different [scikit-learn scalers](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
        "- Try [scikit-learn pipelines](https://scikit-learn.org/stable/modules/compose.html):\n",
        "\n",
        "> Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\n",
        "\n",
        "> - **Convenience and encapsulation.** You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
        "> - **Joint parameter selection.** You can grid search over parameters of all estimators in the pipeline at once.\n",
        "> - **Safety.** Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIZUt3JBdCz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "LOCAL = '../data/tanzania/'\n",
        "WEB = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/tanzania/'\n",
        "\n",
        "train_features = pd.read_csv(WEB + 'train_features.csv')\n",
        "train_labels = pd.read_csv(WEB + 'train_labels.csv')\n",
        "test_features = pd.read_csv(WEB + 'test_features.csv')\n",
        "sample_submission = pd.read_csv(WEB + 'sample_submission.csv')\n",
        "\n",
        "assert train_features.shape == (59400, 40)\n",
        "assert train_labels.shape == (59400, 2)\n",
        "assert test_features.shape == (14358, 40)\n",
        "assert sample_submission.shape == (14358, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "344OIowDoxXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}