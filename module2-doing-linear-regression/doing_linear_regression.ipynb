{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4i5n883p_Xf"
   },
   "source": [
    "_Lambda School Data Science — Linear Models_\n",
    "\n",
    "# Doing Linear Regression\n",
    "\n",
    "### Objectives\n",
    "- arrange data into X features matrix and y target vector\n",
    "- use scikit-learn for linear regression\n",
    "- use regression metric: MAE\n",
    "- do one-hot encoding\n",
    "- scale features\n",
    "\n",
    "### Contents\n",
    "1. Libraries\n",
    "2. Pre-read\n",
    "3. Process\n",
    "4. Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JHJ8JQjGTYro"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWZlqRnfl4sK"
   },
   "source": [
    "### Install [category_encoders](http://contrib.scikit-learn.org/categorical-encoding/) (version 2+)\n",
    "- Local Anaconda: `conda install -c conda-forge category_encoders`\n",
    "- Google Colab: `pip install category_encoders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YS2w89kdl_2g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/a1/f7a22f144f33be78afeb06bfa78478e8284a64263a3c09b1ef54e673841e/category_encoders-2.0.0-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 794kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.21.1 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from category_encoders) (0.24.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from category_encoders) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from category_encoders) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from category_encoders) (1.2.1)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from category_encoders) (0.9.0)\n",
      "Requirement already satisfied: patsy>=0.4.1 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pandas>=0.21.1->category_encoders) (2.8.0)\n",
      "Requirement already satisfied: six in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from patsy>=0.4.1->category_encoders) (1.12.0)\n",
      "Installing collected packages: category-encoders\n",
      "Successfully installed category-encoders-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g74Pk4y0mA5b"
   },
   "source": [
    "### Install [pandas-profiling](https://github.com/pandas-profiling/pandas-profiling) (version 2+)\n",
    "- `pip install -U pandas-profiling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qg1pSAHl9Fg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-profiling\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/e6/873da339e5077824552b752d5e297d66fb42fba0ab698b9d8f66b35fe7d8/pandas-profiling-2.1.2.tar.gz (123kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 983kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pandas>=0.19 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pandas-profiling) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib>=1.4 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pandas-profiling) (3.0.3)\n",
      "Requirement already satisfied, skipping upgrade: jinja2>=2.8 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pandas-profiling) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: missingno>=0.4.1 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pandas-profiling) (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: htmlmin>=0.1.12 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pandas-profiling) (0.1.12)\n",
      "Requirement already satisfied, skipping upgrade: phik>=0.9.8 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pandas-profiling) (0.9.8)\n",
      "Requirement already satisfied, skipping upgrade: confuse>=1.0.0 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pandas-profiling) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pandas>=0.19->pandas-profiling) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pandas>=0.19->pandas-profiling) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pandas>=0.19->pandas-profiling) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from matplotlib>=1.4->pandas-profiling) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from matplotlib>=1.4->pandas-profiling) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from matplotlib>=1.4->pandas-profiling) (2.3.1)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from jinja2>=2.8->pandas-profiling) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: seaborn in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from missingno>=0.4.1->pandas-profiling) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from missingno>=0.4.1->pandas-profiling) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: pytest-pylint>=0.13.0 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from phik>=0.9.8->pandas-profiling) (0.14.0)\n",
      "Requirement already satisfied, skipping upgrade: nbconvert>=5.3.1 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from phik>=0.9.8->pandas-profiling) (5.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numba>=0.38.1 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from phik>=0.9.8->pandas-profiling) (0.43.1)\n",
      "Requirement already satisfied, skipping upgrade: pytest>=4.0.2 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from phik>=0.9.8->pandas-profiling) (4.3.1)\n",
      "Requirement already satisfied, skipping upgrade: jupyter-client>=5.2.3 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from phik>=0.9.8->pandas-profiling) (5.2.4)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from confuse>=1.0.0->pandas-profiling) (5.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas>=0.19->pandas-profiling) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4->pandas-profiling) (40.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pylint>=1.4.5 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (2.3.1)\n",
      "Requirement already satisfied, skipping upgrade: mistune>=0.8.1 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.8.4)\n",
      "Requirement already satisfied, skipping upgrade: pygments in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (2.3.1)\n",
      "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (4.3.2)\n",
      "Requirement already satisfied, skipping upgrade: jupyter_core in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: nbformat>=4.4 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.3)\n",
      "Requirement already satisfied, skipping upgrade: bleach in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (1.4.2)\n",
      "Requirement already satisfied, skipping upgrade: testpath in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: defusedxml in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: llvmlite>=0.28.0dev0 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from numba>=0.38.1->phik>=0.9.8->pandas-profiling) (0.28.0)\n",
      "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (19.1.0)\n",
      "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pluggy>=0.7 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (6.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pyzmq>=13 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (18.0.0)\n",
      "Requirement already satisfied, skipping upgrade: tornado>=4.1 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (6.0.2)\n",
      "Requirement already satisfied, skipping upgrade: astroid<3,>=2.2.0 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (2.2.5)\n",
      "Requirement already satisfied, skipping upgrade: isort<5,>=4.2.5 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (4.3.16)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7,>=0.6 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: ipython-genutils in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from traitlets>=4.2->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from traitlets>=4.2->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (4.4.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from nbformat>=4.4->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: webencodings in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from bleach->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: wrapt in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (1.11.1)\n",
      "Requirement already satisfied, skipping upgrade: typed-ast>=1.3.0; implementation_name == \"cpython\" in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: lazy-object-proxy in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /Users/ashwin/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.14.11)\n",
      "Building wheels for collected packages: pandas-profiling\n",
      "  Building wheel for pandas-profiling (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/ashwin/Library/Caches/pip/wheels/45/10/b5/5565b02c204a3bb87490a22e0e4c382ea06337bb79fd74d58a\n",
      "Successfully built pandas-profiling\n",
      "Installing collected packages: pandas-profiling\n",
      "  Found existing installation: pandas-profiling 2.1.0\n",
      "    Uninstalling pandas-profiling-2.1.0:\n",
      "      Successfully uninstalled pandas-profiling-2.1.0\n",
      "Successfully installed pandas-profiling-2.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pandas-profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6O0KEmmQ7OM"
   },
   "source": [
    "# Pre-reads\n",
    "\n",
    "#### [Jake VanderPlas, Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html). \n",
    "\n",
    "Read up through “Supervised learning example: Simple linear regression”. You can stop when you get to “Supervised learning example: Iris classification.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZ7Oh030150T"
   },
   "source": [
    "# Process\n",
    "\n",
    "#### Renee Teate, [Becoming a Data Scientist, PyData DC 2016 Talk](https://www.becomingadatascientist.com/2016/10/11/pydata-dc-2016-talk/)\n",
    "\n",
    "![](https://image.slidesharecdn.com/becomingadatascientistadvice-pydatadc-shared-161012184823/95/becoming-a-data-scientist-advice-from-my-podcast-guests-55-638.jpg?cb=1476298295)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IEZu7RSd0O3w"
   },
   "source": [
    "## Business Question --> Data Question --> Data Answer (for Supervised Learning)\n",
    "\n",
    "#### Francois Chollet, [Deep Learning with Python](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/README.md), Chapter 4: Fundamentals of machine learning, \"A universal workflow of machine learning\"\n",
    " \n",
    "> **1. Define the problem at hand and the data on which you’ll train.** Collect this data, or annotate it with labels if need be.\n",
    "\n",
    "> **2. Choose how you’ll measure success on your problem.** Which metrics will you monitor on your validation data?\n",
    "\n",
    "> **3. Determine your evaluation protocol:** hold-out validation? K-fold validation? Which portion of the data should you use for validation?\n",
    "\n",
    "> **4. Develop a first model that does better than a basic baseline:** a model with statistical power.\n",
    "\n",
    "> **5. Develop a model that overfits.** The universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting and overfitting; between undercapacity and overcapacity. To figure out where this border lies, first you must cross it.\n",
    "\n",
    "> **6. Regularize your model and tune its hyperparameters, based on performance on the validation data.** Repeatedly modify your model, train it, evaluate on your validation data (not the test data, at this point), modify it again, and repeat, until the model is as good as it can get. \n",
    "\n",
    "> **Iterate on feature engineering: add new features, or remove features that don’t seem to be informative.** Once you’ve developed a satisfactory model configuration, you can train your final production model on all the available data (training and validation) and evaluate it one last time on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJhnVFGQRXS0"
   },
   "source": [
    "## Define the data on which you'll train / Add new features or remove features\n",
    "\n",
    "#### Jake VanderPlas, [Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html), Data Representation in Scikit-Learn\n",
    "\n",
    "> The best way to think about data within Scikit-Learn is in terms of tables of data.\n",
    "\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.02-samples-features.png)\n",
    "\n",
    "> The samples (i.e., rows) always refer to the individual objects described by the dataset. For example, the sample might be a flower, a person, a document, an image, a sound file, a video, an astronomical object, or anything else you can describe with a set of quantitative measurements.\n",
    "\n",
    "> The features (i.e., columns) always refer to the distinct observations that describe each sample in a quantitative manner. \n",
    "\n",
    "> The information can be thought of as a two-dimensional numerical array or matrix, which we will call the _features matrix._ By convention, this features matrix is often stored in a variable named `X`. The features matrix is assumed to be two-dimensional, with shape `[n_samples, n_features]`, and is most often contained in a NumPy array or a Pandas `DataFrame`, though some Scikit-Learn models also accept SciPy sparse matrices.\n",
    "\n",
    "> In addition to the feature matrix `X`, we also generally work with a label or target array, which by convention we will usually call `y`. The target array is usually one dimensional, with length `n_samples`, and is generally contained in a NumPy array or Pandas `Series`. \n",
    "\n",
    "> Often one point of confusion is how the target array differs from the other features columns. The distinguishing feature of the target array is that it is usually the quantity we want to _predict from the data:_ in statistical terms, it is the dependent variable.\n",
    "\n",
    "#### Google Developers, [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/#l) \n",
    "\n",
    "> Each example in a labeled dataset consists of one or more features and a label. \n",
    "\n",
    "> For instance, in a housing dataset, the features might include the number of bedrooms, the number of bathrooms, and the age of the house, while the label might be the house's price. \n",
    "\n",
    "> In a spam detection dataset, the features might include the subject line, the sender, and the email message itself, while the label would probably be either \"spam\" or \"not spam.\"\n",
    "\n",
    "#### Wikipedia, [Feature Engineering](https://en.wikipedia.org/wiki/Feature_engineering)\n",
    "\n",
    "> \"Some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.\" — Pedro Domingos, [\"A Few Useful Things to Know about Machine Learning\"](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "\n",
    "> \"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\" — Andrew Ng, [Machine Learning and AI via Brain simulations](https://forum.stanford.edu/events/2011/2011slides/plenary/2011plenaryNg.pdf) \n",
    "\n",
    "> Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSZBdwNg1Vvj"
   },
   "source": [
    "## Determine evaluation protocol\n",
    "\n",
    "#### Sebastian Raschka, [Model Evaluation]( https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html)\n",
    "> <img src=\"https://sebastianraschka.com/images/blog/2018/model-evaluation-selection-part4/model-eval-conclusions.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFAHa5dD3BQt"
   },
   "source": [
    "## Develop a first model that does better than a basic baseline\n",
    "\n",
    "### Why begin with baselines?\n",
    "\n",
    "[My mentor](https://www.linkedin.com/in/jason-sanchez-62093847/) [taught me](https://youtu.be/0GrciaGYzV0?t=40s):\n",
    "\n",
    ">***Your first goal should always, always, always be getting a generalized prediction as fast as possible.*** You shouldn't spend a lot of time trying to tune your model, trying to add features, trying to engineer features, until you've actually gotten one prediction, at least. \n",
    "\n",
    "> The reason why that's a really good thing is because then ***you'll set a benchmark*** for yourself, and you'll be able to directly see how much effort you put in translates to a better prediction. \n",
    "\n",
    "> What you'll find by working on many models: some effort you put in, actually has very little effect on how well your final model does at predicting new observations. Whereas some very easy changes actually have a lot of effect. And so you get better at allocating your time more effectively.\n",
    "\n",
    "My mentor's advice is echoed and elaborated in several sources:\n",
    "\n",
    "[Always start with a stupid model, no exceptions](https://blog.insightdatascience.com/always-start-with-a-stupid-model-no-exceptions-3a22314b9aaa)\n",
    "\n",
    "> Why start with a baseline? A baseline will take you less than 1/10th of the time, and could provide up to 90% of the results. A baseline puts a more complex model into context. Baselines are easy to deploy.\n",
    "\n",
    "[Measure Once, Cut Twice: Moving Towards Iteration in Data Science](https://blog.datarobot.com/measure-once-cut-twice-moving-towards-iteration-in-data-science)\n",
    "\n",
    "> The iterative approach in data science starts with emphasizing the importance of getting to a first model quickly, rather than starting with the variables and features. Once the first model is built, the work then steadily focuses on continual improvement.\n",
    "\n",
    "[*Data Science for Business*](https://books.google.com/books?id=4ZctAAAAQBAJ&pg=PT276), Chapter 7.3: Evaluation, Baseline Performance, and Implications for Investments in Data\n",
    "\n",
    "> *Consider carefully what would be a reasonable baseline against which to compare model performance.* This is important for the data science team in order to understand whether they indeed are improving performance, and is equally important for demonstrating to stakeholders that mining the data has added value.\n",
    "\n",
    "### What does baseline mean?\n",
    "\n",
    "Baseline is an overloaded term, as you can see in the links above. Baseline has multiple meanings:\n",
    "\n",
    "#### The score you'd get by guessing a single value\n",
    "\n",
    "> A baseline for classification can be the most common class in the training dataset.\n",
    "\n",
    "> A baseline for regression can be the mean of the training labels. —[Will Koehrsen](https://twitter.com/koehrsen_will/status/1088863527778111488)\n",
    "\n",
    "#### The score you'd get by guessing in a more granular way\n",
    "\n",
    "> A baseline for time-series regressions can be the value from the previous timestep.\n",
    "\n",
    "#### Fast, first models that beat guessing\n",
    "\n",
    "What my mentor was talking about.\n",
    "\n",
    "#### Complete, tuned \"simpler\" model\n",
    "\n",
    "Can be simpler mathematically and computationally. For example, Logistic Regression versus Deep Learning.\n",
    "\n",
    "Or can be simpler for the data scientist, with less work. For example, a model with less feature engineering versus a model with more feature engineering.\n",
    "\n",
    "#### Minimum performance that \"matters\"\n",
    "\n",
    "To go to production and get business value.\n",
    "\n",
    "#### Human-level performance \n",
    "\n",
    "Your goal may to be match, or nearly match, human performance, but with better speed, cost, or consistency.\n",
    "\n",
    "Or your goal may to be exceed human performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aOW2hsiDBIk9"
   },
   "source": [
    "## Use scikit-learn to fit a model\n",
    "\n",
    "#### Jake VanderPlas, [Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html), Scikit-Learn's Estimator API\n",
    "\n",
    "> Most commonly, the steps in using the Scikit-Learn estimator API are as follows (we will step through a handful of detailed examples in the sections that follow).\n",
    "\n",
    "> 1. Choose a class of model by importing the appropriate estimator class from Scikit-Learn. \n",
    "> 2. Choose model hyperparameters by instantiating this class with desired values. \n",
    "> 3. Arrange data into a features matrix and target vector following the discussion above.\n",
    "> 4. Fit the model to your data by calling the `fit()` method of the model instance.\n",
    "> 5. Apply the Model to new data: For supervised learning, often we predict labels for unknown data using the `predict()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T1hPspa5mkWT"
   },
   "source": [
    "# Project: Predict NYC apartment rent 🏠💸\n",
    "\n",
    "You'll use a real-world data with rent prices for a subset of apartments in New York City!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UbqeAzl9TK_y"
   },
   "source": [
    "## Define the data on which you'll train\n",
    "\n",
    "- Get the data\n",
    "- What's the target?\n",
    "- Regression or classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qEeVFTa0VWDE"
   },
   "outputs": [],
   "source": [
    "LOCAL = '../data/nyc/nyc-rent-2016.csv'\n",
    "WEB = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/nyc/nyc-rent-2016.csv'\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(LOCAL)\n",
    "assert df.shape == (48300, 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGzhihMGfp1p"
   },
   "outputs": [],
   "source": [
    "df['created'] = pd.to_datetime(df['created'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nnJ6ioiesWsT"
   },
   "source": [
    "## DO TRAIN/TEST SPLIT\n",
    " \n",
    " For this project, we'll split based on time. \n",
    "\n",
    "- Use data from April & May 2016 to train.\n",
    "- Use data from June 2016 to test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYLE4Dzwskn-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2016-06-24 07:54:24\n",
       "1   2016-06-12 12:19:27\n",
       "2   2016-04-17 03:26:41\n",
       "3   2016-04-18 02:22:02\n",
       "4   2016-04-28 01:32:41\n",
       "Name: created, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['created'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMmPjCEWTXFn"
   },
   "source": [
    "## Begin with baselines for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ap063j12tEWJ"
   },
   "outputs": [],
   "source": [
    "df['month'] = df['created'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.query('month < 6') # df[df['month'] < 6]\n",
    "test = df.query('month == 6') # df[df['month'] == 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31515, 35), (16785, 35))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16785 16785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_test = test['price']\n",
    "y_pred = [train['price'].mean()]*len(y_test)\n",
    "print(len(y_test), len(y_pred))\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3hDX7yUTbix"
   },
   "source": [
    "## Use scikit-learn for linear regression, with 1 feature\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFWAop61CgCq"
   },
   "source": [
    "Follow the process from Jake VanderPlas, [Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html), Scikit-Learn's Estimator API\n",
    "\n",
    "### Choose a class of model by importing the appropriate estimator class from Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMbOVWEDCfmO"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vATSdu5oD5NQ"
   },
   "source": [
    "### Choose model hyperparameters by instantiating this class with desired values\n",
    "\n",
    "Refer to scikit-learn documentation to see what model hyperparameters you can choose. For example: [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CexmSzauEBnu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEsa7jtHC0L5"
   },
   "source": [
    "### Arrange data into X features matrix and y target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euIG2-5P_sdZ"
   },
   "outputs": [],
   "source": [
    "feature = ['bedrooms']\n",
    "target = 'price'\n",
    "\n",
    "x_train = train[feature]\n",
    "y_train = train[target]\n",
    "\n",
    "x_test = test[feature]\n",
    "y_test = test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31515, 1), (31515,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8s3-WYWEKxN"
   },
   "source": [
    "### Fit the model to your data by calling the `fit()` method of the model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTLnEzwUENb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HynZZRL7ESvx"
   },
   "source": [
    "### Apply the Model to new data: For supervised learning, often we predict labels for unknown data using the `predict()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mo6h1SnMr9kP"
   },
   "outputs": [],
   "source": [
    "n_bedrooms = 1\n",
    "model.predict([n_bedrooms])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6l-WbivHIwHh"
   },
   "source": [
    "## Use regression metric: MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5G7hDzGG0B0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_KGM3LOHyrW"
   },
   "source": [
    "## Use scikit-learn for linear regression, with 2 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_79qOeAH2ZU"
   },
   "source": [
    "Follow the process from Jake VanderPlas, [Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html), Scikit-Learn's Estimator API\n",
    "\n",
    "### Choose a class of model by importing the appropriate estimator class from Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E26qduGiH2_y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "--7julXYH3oC"
   },
   "source": [
    "### Choose model hyperparameters by instantiating this class with desired values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BkkoMxbsIXLR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJNRFvK9IeWU"
   },
   "source": [
    "### Arrange data into X features matrix and y target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2290BJszIgrb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOa5Uj4jIjDR"
   },
   "source": [
    "### Fit the model to your data by calling the `fit()` method of the model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZAUSsY0IjWa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GEyW2B3Imr2"
   },
   "source": [
    "### Apply the Model to new data: For supervised learning, often we predict labels for unknown data using the `predict()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ubKZVRJInLV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDbG8jreI8Ip"
   },
   "source": [
    "## Use regression metric: MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCs--47RI-He"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BM-KUzX8RqsS"
   },
   "source": [
    "## Do one-hot encoding of categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VqeBG7OoR72b"
   },
   "source": [
    "### Which features are non-numeric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ic36yUPRp2l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aN89gf1QSJG3"
   },
   "source": [
    "### Check \"cardinality\" of non-numeric features\n",
    "\n",
    "[Cardinality](https://simple.wikipedia.org/wiki/Cardinality) means the number of unique values that a feature has:\n",
    "> In mathematics, the cardinality of a set means the number of its elements. For example, the set A = {2, 4, 6} contains 3 elements, and therefore A has a cardinality of 3. \n",
    "\n",
    "\"One-hot encoding\" adds a dimension for each unique value of each categorical feature. So, it may not be a good choice for \"high cardinality\" categoricals that have dozens, hundreds, or thousands of unique values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IkmTd6W3SNey"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ApYuyrb8SaDp"
   },
   "source": [
    "### Explore `interest_level` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipS3vkvcRwub"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QGM1e6ThSp5K"
   },
   "source": [
    "### Encode `interest_level` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28TRmEX_SuzM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V15ZztGdccx5"
   },
   "source": [
    "## Do one-hot encoding & Scale features, \n",
    "within a complete model fitting workflow.\n",
    "\n",
    "### Why and how to scale features before fitting linear models\n",
    "\n",
    "Scikit-Learn User Guide, [Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "> Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "\n",
    "> The `preprocessing` module further provides a utility class `StandardScaler` that implements the `Transformer` API to compute the mean and standard deviation on a training set. The scaler instance can then be used on new data to transform it the same way it did on the training set.\n",
    "\n",
    "### How to use encoders and scalers in scikit-learn\n",
    "- Use the **`fit_transform`** method on the **train** set\n",
    "- Use the **`transform`** method on the **validation** set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jf6r8sCDccDv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70KA2iiTao3Z"
   },
   "source": [
    "# Assignment\n",
    "1. Start a clean notebook. Follow the processes taught today.\n",
    "2. Do train/test split. Use data from April & May 2016 to train. Use data from June 2016 to test.\n",
    "3. Begin with baseline for regression.\n",
    "4. Select two or more features. \n",
    "5. Do one-hot encoding. (Remember it may not work with high cardinality categoricals.)\n",
    "4. Use scikit-learn to fit a Linear Regression model on the train data.\n",
    "5. Apply the model to predict rent prices for the test data.\n",
    "6. Get the mean absolute error for the test data.\n",
    "7. Get the model's coefficients and intercept.\n",
    "8. Commit your notebook to your fork of the GitHub repo.\n",
    "\n",
    "_What's the best test MAE you can get? Share your score and features used with your cohort on Slack!_\n",
    "\n",
    "### Stretch Goals\n",
    "- Try at least 3 different feature combinations.\n",
    "- Get regression metrics RMSE, MAE, and $R^2$, for both the train and test data.\n",
    "- [Engineer new features!](https://en.wikipedia.org/wiki/Feature_engineering)\n",
    "\n",
    "> \"Some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.\" — Pedro Domingos, [\"A Few Useful Things to Know about Machine Learning\"](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "\n",
    "> \"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\" — Andrew Ng, [Machine Learning and AI via Brain simulations](https://forum.stanford.edu/events/2011/2011slides/plenary/2011plenaryNg.pdf) \n",
    "\n",
    "> Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. \n",
    "\n",
    "- Try different [scikit-learn scalers](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- Try [scikit-learn pipelines](https://scikit-learn.org/stable/modules/compose.html):\n",
    "\n",
    "> Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\n",
    "\n",
    "> - **Convenience and encapsulation.** You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "> - **Joint parameter selection.** You can grid search over parameters of all estimators in the pipeline at once.\n",
    "> - **Safety.** Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "doing_linear_regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
